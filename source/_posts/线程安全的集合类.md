---
title: 线程安全的集合类和一些其它知识点
date: 2021-11-25 22:08:12
categories: 并发编程
tags: 并发编程
urlname: java-concurrent-7
---

线程安全集合类可以分为三大类：

遗留的线程安全集合如 Hashtable ， Vector

使用 Collections 装饰的线程安全集合，如：

- Collections.synchronizedCollection
- Collections.synchronizedList
- Collections.synchronizedMap
- Collections.synchronizedSet
- Collections.synchronizedNavigableMap
- Collections.synchronizedNavigableSet 
- Collections.synchronizedSortedMap
- Collections.synchronizedSortedSet

java.util.concurrent.*

<!--more-->

重点介绍 java.util.concurrent.* 下的线程安全集合类，可以发现它们有规律，里面包含三类关键词：Blocking、CopyOnWrite、Concurrent

- Blocking 大部分实现基于锁，并提供用来阻塞的方法
- CopyOnWrite 之类容器修改开销相对较重
- Concurrent 类型的容器
  - 内部很多操作使用 cas 优化，一般可以提供较高吞吐量
  - 弱一致性
    - 遍历时弱一致性，例如，当利用迭代器遍历时，如果容器发生修改，迭代器仍然可以继续进行遍历，这时内容是旧的
    - 求大小弱一致性，size 操作未必是 100% 准确
    - 读取弱一致性

> 遍历时如果发生了修改，对于非安全容器来讲，使用 fail-fast 机制也就是让遍历立刻失败，抛出
> ConcurrentModificationException，不再继续遍历

## ConcurrentHashMap 注意

ConcurrentHashMap 是一个线程安全的集合，ConcurrentHashMap 中的方法，每个方法都能保证在多线程下是原子的，线程安全的，但是业务中有时候会有需求，用 ConcurrentHashMap 中的方法组合使用从而实现某些业务场景，组合去操作同一个共享资源。这样的话，组合的这段代码就可能有线程安全问题。意思就是每个方法是原子的，但是方法组合使用，这段代码就不能保证原子了，其实这儿在前面也提到过。

比如说有一个需求，需要从 map 集合中检查某个 key 是否存在，如果不存在，说明这个 key 是没有的，没有的话需要 put 一个 key，并且给这个 key 对应的 value 一个初始值，将来要累加。

例如下面的代码：

```java
Integer counter = map.get(word);
int newValue = counter == null ? 1 : counter + 1;
map.put(word, newValue);
```

这就是组合使用了。上面的案例中，其实如果在多线程的情况下，ConcurrentHashMap 的 map 对象是一个共享资源，尽管 ConcurrentHashMap 的 get 和 put 方法每个都是原子的，但是这一段代码组合起来就是线程不安全的。

ConcurrentHashMap 提供解决这种场景的方案：

```java
// 注意不能使用 putIfAbsent，此方法返回的是上一次的 value，首次调用返回 null
LongAdder value = map.computeIfAbsent(word, (key) -> new LongAdder());//LongAdder原子累加器，基础是从0开始实现累加效果的
value.increment();// 0-->1、1-->2
```

上面代码中，如果 map 中这个 key 没有，生成一个累加器，将累加器返回；key 有的话，就无需再创建累加器了，一直是同一个累加器来执行这个累加操作的。

## JDK7 中 HashMap 的并发死链

注意，HashMap 的并发死链只是在 JDK7 中才有可能复现，最根本的原因就是并发扩容时的“头插法”。

首先回顾 HashMap 的数据结构：它的数据结构其实就是哈希表，往细了说在 jdk7 中是数组＋链表的结构；在 jdk8 中，就是数组＋链表＋红黑树的结构。

当一个 key 被放入 HashMap 时，首先会计算这个 key 的 hash 值，然后对整个数组的长度进行取模，得到桶下标，然后放入数组下标值与桶下标对应的位置。这样的查找效率非常高，因为是底层数据结构是数组。但是因为数组的容量有限，因此随着存入数量的增加，不可避免会有取模值相同的情况，也就是所谓的“桶下标冲突”，有可能 key 是值不相同的 key 但是它们的桶下标相同，这时 jdk 会让桶下标相同的 key 在数组的同一索引位置形成一个链表，来解决桶下标冲突。将来查找 key 时，先得到 key 的 hash 值，不冲突的情况下直接返回，一旦发生了桶下标冲突，那么再在链表头用 equals 一个一个比较，来查找这个 key。虽然性能上稍微有所损失，但是可解决桶下标冲突。这便是 JDK7 的 HashMap 的基本结构。

有必要说一下：JDK7 为链表头插法，也就是如果没有桶下标冲突，那么数组每个索引的数据都好比链表的头节点，当有了冲突的时候，这个新的冲突的 key 会放在对应索引对应链表的头节点的位置，也就是数组和链表交界的位置节点。但是 JDK8 中，为链表的尾插法，当有了冲突的时候，这个新的冲突的 key 会放在对应索引对应链表的尾节点的位置。

接下来说一说扩容的问题，JDK8 中除了对插入方式进行了改善，并且对扩容策略也进行了升级。首先，随着元素越来越多，必然导致链的长度越来越大，进而查找性能必然受到影响，因为数组结构是查询快，增删慢（因为需要考虑复制成本），链表是增删快，查询慢，所以链表的长度越长，查找的性能越慢。因此为了解决这个问题，JDK 会在数组元素超过阈值（0.75）时，进行一次扩容。

Hashmap的扩容需要满足两个条件：当前数据存储的数量（即size()）大小必须大于等于阈值；当前加入的数据是否发生了hash冲突。因为上面这两个条件，所以存在下面这些情况

（1）、就是hashmap在存值的时候（默认大小为16，负载因子0.75，阈值12），可能达到最后存满16个值的时候，再存入第17个值才会发生扩容现象，因为前16个值，每个值在底层数组中分别占据一个位置，并没有发生hash碰撞。

（2）、当然也有可能存储更多值（超多16个值，最多可以存26个值）都还没有扩容。原理：前11个值全部hash碰撞，存到数组的同一个位置（这时元素个数小于阈值12，不会扩容），后面所有存入的15个值全部分散到数组剩下的15个位置（这时元素个数大于等于阈值，但是每次存入的元素并没有发生hash碰撞，所以不会扩容），前面11+15=26，所以在存入第27个值的时候才同时满足上面两个条件，这时候才会发生扩容现象。

扩容实际上就是产生一个新的数组，容量是原来数组的2倍，并且扩容会重新计算桶下标，原来的链表中的一个一个元素会迁移到新的数组中。扩容后链表的长度缩短，查找性能提升。但是在 JDK7 中也就是因为在多线程下进行扩容时，可能会造成并发死链的问题，这里在 jdk8 中改用了尾插法得到了避免。另外，JDK8 中对于扩容做了改善。HashMap 数组的初始长度为16，当发生扩容时，数组长度最大扩容到64之后，如果链表的长度还是 >= 8（树化阈值），将进行链表转化为红黑树的操作。

HashSet 是不允许存入重复的数据的，如果存入的数据之前存过了，那么 add 方法会返回 false， HashMap 是不允许存入相同的 key 值，如果存的 key 之前存过了相同的 key，那么这后面存的键值对会覆盖前面存的键值对，比如 key valueNew 会覆盖 key valueOld，就相当于更改。那底层是怎么判断重复的呢？比如新存一个值，那么 HashSet 底层会先求出这个值的 Hash值，然后在数组中查找，是否有同样的 Hash 值的元素，如果没有直接存入，如果有相同的哈希值那么进而使用 equals 判断，看这个哈希值对应的链表上是否有 equals 判断后返回 true 的节点，有的话证明之前存过相同的值，那么 add 方法返回 false，没有的话证明之前没有存过，add 方法返回 true。或者新存一个键值对，那么在 HashMap 中就是判断 key 值，原理和 HashSet 一致。

## HashMap 详细剖析

问：HashMap 的底层数据结构，在 1.7 与 1.8 中有何不同？

### 1）基本数据结构

* 1.7 数组 + 链表
* 1.8 数组 + （链表 | 红黑树）

问：为何要用红黑树？为何一上来不树化？树化阈值为何是 8？何时会树化？何时会退化为链表？

### 2）树化与退化

**树化意义**

* 红黑树用来避免 DoS 攻击，防止链表超长时性能下降，树化应当是偶然情况，是保底策略
* hash 表的查找，更新的时间复杂度是 $O(1)$，而红黑树的查找，更新的时间复杂度是 $O(log_2⁡n )$，TreeNode 占用空间也比普通 Node 的大，并且长度短的链表在挨个查找时的效率不见得比红黑树差。因此如非必要，尽量还是使用链表
* hash 值如果足够随机，则在 hash 表内按泊松分布，在负载因子 0.75 的情况下，长度超过 8 的链表出现概率是 0.00000006，树化阈值选择 8 就是为了让树化几率足够小

**树化规则**

* 当链表长度超过树化阈值 8 时，先尝试扩容来减少链表长度，如果数组容量已经 >=64，才会进行树化

**退化规则**

* 情况1：在扩容时，如果拆分树了，导致树元素个数 <= 6 时，树则会退化为链表
* 情况2：remove 树节点时，若 root、root.left、root.right、root.left.left 有一个为 null ，也会退化为链表



阶段小总结：

HashMap 中存入的是 key-value 结构的键值对，key 是不可重复的，如果重复那么这个 key 对应的新的 value 就会覆盖原来的 value。

HashMap 在 1.8 中，是数组 + （链表 | 红黑树）的底层结构。在 HashMap 存入键值对的时候，是根据 key 来做运算的，对 key 进行 hashCode() 运算，再进行调用 HashMap 的 hash() 方法进行二次哈希，得到哈希值，然后让这个哈希值与底层数组的初始长度（16）取模运算，得到桶下标，也就是对应底层数组的索引位置，然后就将这个 key 放到这个索引位置。如果出现哈希碰撞的情况，也就是有可能在数组的同一个索引处放入多个值，那么这些值就组成了一个链表。如果发生哈希碰撞的值较多，链表的长度会逐渐增加，但是链表超长时性能下降，因此等链表达到一定长度（8）时，再向这个链表对应的索引放入数据时，会先尝试扩容数组（+16）来减少链表长度，但如果数组长度已经 >=64 时，此时链表就会转换为红黑树。

数组扩容的两个情况：

1. 负载因子 0.75：HashMap 中 key 的数量大于底层数组长度的 3/4 时，扩容。
2. 链表达到一定长度（8），并且数组长度 ＜64 时，再向这个链表对应的索引放入数据时，可能此时没达到 0.75 阈值，但也会先尝试扩容数组（+16）来减少链表长度。

树化条件（必须同时满足下面两个）：

- 当链表长度超过树化阈值 8 
- 数组长度已经 >=64

树退化回链表的两种情况：

* 情况1：在扩容时，如果拆分树了，导致树元素个数 <= 6 时，树则会退化为链表
* 情况2：remove 树节点时，若 root、root.left、root.right、root.left.left 有一个为 null 时，也会退化为链表



问：索引如何计算？hashCode 都有了，为何还要提供 hash() 方法？数组容量为何是 2 的 n 次幂？

### 3）索引计算

**索引计算方法**

* 首先，计算对象的 hashCode()
* 再进行调用 HashMap 的 hash() 方法进行二次哈希
  * 二次 hash() 是为了综合高位数据，让哈希分布更为均匀
* 最后`哈希值 % capacity` 得到索引。或者 `哈希值 & (capacity - 1)` 按位与运算代替取模运算，因为按位与运算效率偏高，但是要注意，只有在 capacity 是 2 的 n 次幂的时候，才可以使用位与运算代替取模。

**数组容量为何是 2 的 n 次幂**

1. 计算索引时效率更高：如果是 2 的 n 次幂可以使用位与运算代替取模
2. 扩容时重新计算索引效率更高： hash & oldCap == 0 的元素留在原来位置 ，否则新位置 = 旧位置 + oldCap

**注意**

* 二次 hash 是为了配合 **容量是 2 的 n 次幂** 这一设计前提，如果 hash 表的容量不是 2 的 n 次幂，则不必二次 hash
* **容量是 2 的 n 次幂** 这一设计计算索引效率更好，但 hash 的分散性就不好，需要二次 hash 来作为补偿，没有采用这一设计的典型例子是 Hashtable



问：介绍一下 put 方法流程，1.7 与 1.8 有何不同？

### 4）put 与扩容

**put 流程**

1. HashMap 是懒惰创建数组的（并不是一上来就创建一个长度为16的数组），首次使用（调用 put 方法时），才创建数组
2. 计算索引（桶下标）
3. 如果桶下标还没人占用，创建 Node 占位返回
4. 如果桶下标已经有人占用
   1. 已经是 TreeNode 走红黑树的添加或更新逻辑
   2. 是普通 Node，走链表的添加或更新逻辑，如果链表长度超过树化阈值，根据树化规则走树化逻辑
5. 返回前检查容量是否超过阈值，一旦超过进行扩容（也就是先把元素加到旧数组，然后扩容迁移元素）

**1.7 与 1.8 的区别**

1. 链表插入节点时，1.7 是头插法，1.8 是尾插法

2. 1.7 是大于等于阈值且没有空位（表示新加的元素要加到的索引位置上是空着的）时才扩容，而 1.8 是大于阈值就扩容

3. 1.8 在扩容计算 Node 索引时，会优化（优化方案：将哈希值跟旧的数组长度做按位与，如果结果是0，表示这个元素在扩容后不用动位置，如果不是0，表示这个元素在扩容之后要移动到一个新位置，新位置 = 这个元素的旧的索引 + 旧的数组长度）

问：**扩容（加载）因子为何默认是 0.75f**？

1. 在空间占用与查询时间之间取得较好的权衡
2. 大于这个值，空间节省了，但链表就会比较长影响性能
3. 小于这个值，冲突减少了，但扩容就会更频繁，空间占用也更多

### 5）并发问题

**扩容死链（1.7 会存在）**

1.7 源码如下：

```java
void transfer(Entry[] newTable, boolean rehash) {
    int newCapacity = newTable.length;
    for (Entry<K,V> e : table) {
        while(null != e) {
            Entry<K,V> next = e.next;
            if (rehash) {
                e.hash = null == e.key ? 0 : hash(e.key);
            }
            int i = indexFor(e.hash, newCapacity);
            e.next = newTable[i];
            newTable[i] = e;
            e = next;
        }
    }
}
```

* e 和 next 都是局部变量，用来指向当前节点和下一个节点
* 线程1（绿色）的临时变量 e 和 next 刚引用了这俩节点，还未来得及移动节点，此时发生了线程切换，线程 2 得到了 CPU 执行权，由线程2（蓝色）完成扩容和迁移

注：图中 e 表示当前要迁移的节点，next 表示下一个节点。

注：每轮迁移结束会把 e 指向 next 所指向的节点，下一轮开始会把 next 指向当前节点的下一个节点。

![image-20210831084325075](https://blog-images-erdochan.oss-cn-beijing.aliyuncs.com/img/image-20210831084325075.png)

* 线程2 扩容完成，并完成迁移，但由于头插法，链表顺序颠倒。但此时线程 1 的临时变量 e 和 next 依然正在引用这俩节点，因此当线程 1 重新得到 CPU 的执行权之后，还要再来一遍迁移。

![image-20210831084723383](https://blog-images-erdochan.oss-cn-beijing.aliyuncs.com/img/image-20210831084723383.png)

* 第一次循环
  * 循环接着线程切换前运行，注意此时 e 指向的是节点 a，next 指向的是节点 b
  * e 头插 a 节点，注意图中画了两份 a 节点，但事实上只有一个（为了不让箭头特别乱画了两份）
  * 当循环结束是 e 会指向 next 也就是 b 节点

![image-20210831084855348](https://blog-images-erdochan.oss-cn-beijing.aliyuncs.com/img/image-20210831084855348.png)

* 第二次循环
  * next 指向了节点 a
  * e 头插节点 b
  * 当循环结束时，e 指向 next 也就是节点 a

![image-20210831085329449](https://blog-images-erdochan.oss-cn-beijing.aliyuncs.com/img/image-20210831085329449.png)

* 第三次循环
  * next 指向了 null
  * e 头插节点 a，**a 的 next 指向了 b**（之前 a.next 一直是 null），b 的 next 指向 a，死链已成
  * 当循环结束时，e 指向 next 也就是 null，因此第四次循环时会正常退出

![image-20210831085543224](https://blog-images-erdochan.oss-cn-beijing.aliyuncs.com/img/image-20210831085543224.png)

**数据错乱（1.7，1.8 都会存在）**

多线程下，如果是多个线程并发操作 HashMap，向其中 putVal，有可能造成数据丢失的。

* 代码参考 `day01.map.HashMapMissData`，具体调试步骤参考视频

> ***补充代码说明***
>
> * day01.map.HashMapDistribution 演示 map 中链表长度符合泊松分布
> * day01.map.DistributionAffectedByCapacity 演示容量及 hashCode 取值对分布的影响
>   * day01.map.DistributionAffectedByCapacity#hashtableGrowRule 演示了 Hashtable 的扩容规律
>   * day01.sort.Utils#randomArray 如果 hashCode 足够随机，容量是否是 2 的 n 次幂影响不大
>   * day01.sort.Utils#lowSameArray 如果 hashCode 低位一样的多，容量是 2 的 n 次幂会导致分布不均匀
>   * day01.sort.Utils#evenArray 如果 hashCode 偶数的多，容量是 2 的 n 次幂会导致分布不均匀
>   * 由此得出对于容量是 2 的 n 次幂的设计来讲，二次 hash 非常重要
> * day01.map.HashMapVsHashtable 演示了对于同样数量的单词字符串放入 HashMap 和 Hashtable 分布上的区别



### 6）key 的设计

**key 的设计要求**

1. HashMap 的 key 可以为 null，但 Map 的其他实现则不然
2. 作为 key 的对象，必须实现 hashCode 和 equals，并且 key 的内容不能修改（不可变）。注：两个值，hashCode 相同不一定 equals，但是 equals 的话 hashCode 一定相同。
3. key 的 hashCode 应该有良好的散列性

如果 key 可变，例如修改了 age 会导致再次查询时查询不到

```java
public class HashMapMutableKey {
    public static void main(String[] args) {
        HashMap<Student, Object> map = new HashMap<>();
        Student stu = new Student("张三", 18);
        map.put(stu, new Object());

        System.out.println(map.get(stu));

        stu.age = 19;
        System.out.println(map.get(stu));
    }

    static class Student {
        String name;
        int age;

        public Student(String name, int age) {
            this.name = name;
            this.age = age;
        }

        public String getName() {
            return name;
        }

        public void setName(String name) {
            this.name = name;
        }

        public int getAge() {
            return age;
        }

        public void setAge(int age) {
            this.age = age;
        }

        @Override
        public boolean equals(Object o) {
            if (this == o) return true;
            if (o == null || getClass() != o.getClass()) return false;
            Student student = (Student) o;
            return age == student.age && Objects.equals(name, student.name);
        }

        @Override
        public int hashCode() {
            return Objects.hash(name, age);
        }
    }
}
```



**String 对象的 hashCode() 设计**

* 目标是达到较为均匀的散列效果，每个字符串的 hashCode 足够独特
* 字符串中的每个字符都可以表现为一个数字，称为 $S_i$，其中 i 的范围是 0 ~ n - 1 
* 散列公式为： $S_0∗31^{(n-1)}+ S_1∗31^{(n-2)}+ … S_i ∗ 31^{(n-1-i)}+ …S_{(n-1)}∗31^0$
* 31 代入公式有较好的散列特性，并且 31 * h 可以被优化为 
  * 即 $32 ∗h -h $
  * 即 $2^5  ∗h -h$
  * 即 $h≪5  -h$

### 7）JDK1.8中的线程不安全

为什么说 HashMap 线程不安全？因为并发插入数据时数据可能会被覆盖，那为什么说JDK1.8会出现数据覆盖的情况喃，我们来看一下下面这段JDK1.8中的put操作代码：

```java
final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
        Node<K,V>[] tab; Node<K,V> p; int n, i;
        if ((tab = table) == null || (n = tab.length) == 0)
            n = (tab = resize()).length;
        if ((p = tab[i = (n - 1) & hash]) == null) // 如果没有hash碰撞则直接插入元素，并发不安全
            tab[i] = newNode(hash, key, value, null);
        else {
            Node<K,V> e; K k;
            if (p.hash == hash &&
                ((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            else {
                for (int binCount = 0; ; ++binCount) {
                    if ((e = p.next) == null) {
                        p.next = newNode(hash, key, value, null);
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
            if (e != null) { // existing mapping for key
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;
            }
        }
        ++modCount;
        if (++size > threshold)// 并发不安全
            resize();
        afterNodeInsertion(evict);
        return null;
    }
```

其中第六行代码是判断是否出现hash碰撞，假设两个线程A、B都在进行put操作，并且hash函数计算出的插入下标是相同的，当线程A执行完第六行代码后由于时间片耗尽导致被挂起，而线程B得到时间片后在该下标处插入了元素，完成了正常的插入，然后线程A获得时间片，由于之前已经进行了hash碰撞的判断，所有此时不会再进行判断，而是直接进行插入，这就导致了线程B插入的数据被线程A覆盖了，从而线程不安全。

除此之外，还有就是代码的倒数第五行处有个++size，我们这样想，还是线程A、B，这两个线程同时进行put操作时，假设当前HashMap的zise大小为10，当线程A执行到倒数第五行代码时，从主内存中获得size的值为10，后准备进行+1操作，但是还没等执行+1时，由于时间片耗尽只好让出CPU，线程B快乐的拿到CPU还是从主内存中拿到size的值10进行+1操作，完成了put操作并将size=11写回主内存，然后线程A再次拿到CPU并继续执行(此时size的值仍为10)，当执行完put操作后，还是将size=11写回内存，此时，线程A、B都执行了一次put操作，但是size的值只增加了1，所有说还是由于数据覆盖又导致了线程不安全。

## JDK8 的 ConcurrentHashMap

### 重要属性和内部类

```java
// 默认为 0
// 当初始化时, 为 -1
// 当扩容时, 为 -(1 + 扩容线程数)
// 当初始化或扩容完成后，为 下一次的扩容的阈值大小
private transient volatile int sizeCtl;
// 整个 ConcurrentHashMap 就是一个 Node[]
static class Node<K,V> implements Map.Entry<K,V> {}
// hash 表
transient volatile Node<K,V>[] table;
// 扩容时的 新 hash 表
private transient volatile Node<K,V>[] nextTable;
// 扩容时如果某个 bin 迁移完毕, 用 ForwardingNode 作为旧 table bin 的头结点
static final class ForwardingNode<K,V> extends Node<K,V> {}
// 用在 compute 以及 computeIfAbsent 时, 用来占位, 计算完成后替换为普通 Node
static final class ReservationNode<K,V> extends Node<K,V> {}
// 作为 treebin 的头节点, 存储 root 和 first
static final class TreeBin<K,V> extends Node<K,V> {}
// 作为 treebin 的节点, 存储 parent, left, right
static final class TreeNode<K,V> extends Node<K,V> {}
```

### 重要方法

```java
// 获取 Node[] 中第 i 个 Node
static final <K,V> Node<K,V> tabAt(Node<K,V>[] tab, int i)
 
// cas 修改 Node[] 中第 i 个 Node 的值, c 为旧值, v 为新值
static final <K,V> boolean casTabAt(Node<K,V>[] tab, int i, Node<K,V> c, Node<K,V> v)
 
// 直接修改 Node[] 中第 i 个 Node 的值, v 为新值
static final <K,V> void setTabAt(Node<K,V>[] tab, int i, Node<K,V> v)
```

### 构造器分析

可以看到实现了懒惰初始化，在构造方法中仅仅计算了 table 的大小，以后在第一次使用时才会真正创建

```java
// initialCapacity 初始容量也就是初始大小；loadFactor 负载因子。3/4 表示将来扩容的阈值；concurrencyLevel 并发度。
public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) {
    if (!(loadFactor > 0.0f) || initialCapacity < 0 || concurrencyLevel <= 0)
        throw new IllegalArgumentException();
    // 如果初始容量小于并发度
    if (initialCapacity < concurrencyLevel) // Use at least as many bins
        // 初始容量最小要保证并发度这么大，将并发度的值给初始容量
        initialCapacity = concurrencyLevel; // as estimated threads
    long size = (long)(1.0 + (long)initialCapacity / loadFactor);
    // tableSizeFor 仍然是保证计算的大小是 2^n, 即 16,32,64 ... 
    int cap = (size >= (long)MAXIMUM_CAPACITY) ?
        MAXIMUM_CAPACITY : tableSizeFor((int)size);
    this.sizeCtl = cap;
}
```

JDK8 中的 ConcurrentHashMap 是懒惰初始化的，上来并不是将数组先创建出来，而是将来真正用到的时候才会创建。在 7 中是不管用没用，一上来就会创建一个 segment 数组，这样会增加内存的成本，在 8 中改进了。

### get 流程

```java
public V get(Object key) {
    Node<K,V>[] tab; Node<K,V> e, p; int n, eh; K ek;
    // spread 方法能确保返回结果是正数,因为哈希码可能存在负数的情况
    int h = spread(key.hashCode());// 这个h是进行put或者get时真正用到的哈希码
    // 先判断table，table中如果有元素则往里寻找key，否则直接返回null
    if ((tab = table) != null && (n = tab.length) > 0 &&
        // tabAt方法，找到某一个链表，先定位一个桶下标根据桶下标找到对应的链表
        // 如何找到桶下标？(n-1)&h 数组长-1按位与相当于取模运算，但是按位与效率更高
        // 找到头节点看是否不为空，如果不为空，比较头节点的哈希码是否=刚才的key的哈希码
        (e = tabAt(tab, (n - 1) & h)) != null) {
        if ((eh = e.hash) == h) {
            // 进一步判断这个key是否和我们查找的key是一样的 一样返回value，不一样进一步用equals对key进行判断
            if ((ek = e.key) == key || (ek != null && key.equals(ek)))
                return e.val;
        }
        // hash 为负数表示该 bin 在扩容中或是 treebin, 这时调用 find 方法来查找
        else if (eh < 0)
            return (p = e.find(h, key)) != null ? p.val : null;
        // 正常遍历链表, 用 equals 比较
        while ((e = e.next) != null) {
            if (e.hash == h &&
                ((ek = e.key) == key || (ek != null && key.equals(ek))))
                return e.val;
        }
    }
    return null;
}
```

### put 流程

以下数组简称（table），链表简称（bin）

```java
public V put(K key, V value) {
    return putVal(key, value, false);
}
final V putVal(K key, V value, boolean onlyIfAbsent) {
    if (key == null || value == null) throw new NullPointerException();
    // 其中 spread 方法会综合高位低位, 具有更好的 hash 性
    int hash = spread(key.hashCode());
    int binCount = 0;
    for (Node<K,V>[] tab = table;;) {
        // f 是链表头节点
        // fh 是链表头结点的 hash
        // i 是链表在 table 中的下标
        Node<K,V> f; int n, i, fh;
        // 要创建 table
        if (tab == null || (n = tab.length) == 0)
            // 初始化 table 使用了 cas, 无需 synchronized 创建成功, 进入下一轮循环
            tab = initTable();
        // 要创建链表头节点
        else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) {
            // 添加链表头使用了 cas, 无需 synchronized
            if (casTabAt(tab, i, null,
                         new Node<K,V>(hash, key, value, null)))
                break;
        }
        // 帮忙扩容
        else if ((fh = f.hash) == MOVED)
            // 帮忙之后, 进入下一轮循环
            tab = helpTransfer(tab, f);
        else {
            V oldVal = null;
            // 锁住链表头节点
            synchronized (f) {
                // 再次确认链表头节点没有被移动
                if (tabAt(tab, i) == f) {
                    // 链表
                    if (fh >= 0) {
                        binCount = 1;
                        // 遍历链表
                        for (Node<K,V> e = f;; ++binCount) {
                            K ek;
                            // 找到相同的 key
                            if (e.hash == hash &&
                                ((ek = e.key) == key ||
                                 (ek != null && key.equals(ek)))) {
                                oldVal = e.val;
                                // 更新
                                if (!onlyIfAbsent)
                                    e.val = value;
                                break;
                            }
                            Node<K,V> pred = e;
                            // 已经是最后的节点了, 新增 Node, 追加至链表尾
                            if ((e = e.next) == null) {
                                pred.next = new Node<K,V>(hash, key,
                                                          value, null);
                                break;
                            }
                        }
                    }
                    // 红黑树
                    else if (f instanceof TreeBin) {
                        Node<K,V> p;
                        binCount = 2;
                        // putTreeVal 会看 key 是否已经在树中, 是, 则返回对应的 TreeNode
                        if ((p = ((TreeBin<K,V>)f).putTreeVal(hash, key,
                                                              value)) != null) {
                            oldVal = p.val;
                            if (!onlyIfAbsent)
                                p.val = value;
                        }
                    }
                }
                // 释放链表头节点的锁
            }

            if (binCount != 0) { 
                if (binCount >= TREEIFY_THRESHOLD)
                    // 如果链表长度 >= 树化阈值(8), 进行链表转为红黑树
                    treeifyBin(tab, i);
                if (oldVal != null)
                    return oldVal;
                break;
            }
        }
    }
    // 增加 size 计数
    addCount(1L, binCount);
    return null;
}
private final Node<K,V>[] initTable() {
    Node<K,V>[] tab; int sc;
    while ((tab = table) == null || tab.length == 0) {
        if ((sc = sizeCtl) < 0)
            Thread.yield();
        // 尝试将 sizeCtl 设置为 -1（表示初始化 table）
        else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) {
            // 获得锁, 创建 table, 这时其它线程会在 while() 循环中 yield 直至 table 创建
            try {
                if ((tab = table) == null || tab.length == 0) {
                    int n = (sc > 0) ? sc : DEFAULT_CAPACITY;
                    Node<K,V>[] nt = (Node<K,V>[])new Node<?,?>[n];
                    table = tab = nt;
                    sc = n - (n >>> 2);
                }
            } finally {
                sizeCtl = sc;
            }
            break;
        }
    }
    return tab;
}
// check 是之前 binCount 的个数
private final void addCount(long x, int check) {
    CounterCell[] as; long b, s;
    if (
        // 已经有了 counterCells, 向 cell 累加
        (as = counterCells) != null ||
        // 还没有, 向 baseCount 累加
        !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)
    ) {
        CounterCell a; long v; int m;
        boolean uncontended = true;
        if (
            // 还没有 counterCells
            as == null || (m = as.length - 1) < 0 ||
            // 还没有 cell
            (a = as[ThreadLocalRandom.getProbe() & m]) == null ||
            // cell cas 增加计数失败
            !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))
        ) {
            // 创建累加单元数组和cell, 累加重试
            fullAddCount(x, uncontended);
            return;
        }
        if (check <= 1)
            return;
        // 获取元素个数
        s = sumCount();
    }
    if (check >= 0) {
        Node<K,V>[] tab, nt; int n, sc;
        while (s >= (long)(sc = sizeCtl) && (tab = table) != null &&
               (n = tab.length) < MAXIMUM_CAPACITY) {
            int rs = resizeStamp(n);
            if (sc < 0) {
                if ((sc >>> RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 ||
                    sc == rs + MAX_RESIZERS || (nt = nextTable) == null ||
                    transferIndex <= 0)
                    break;
                // newtable 已经创建了，帮忙扩容
                if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1))
                    transfer(tab, nt);
            }
            // 需要扩容，这时 newtable 未创建
            else if (U.compareAndSwapInt(this, SIZECTL, sc,
                                         (rs << RESIZE_STAMP_SHIFT) + 2))
                transfer(tab, null);
            s = sumCount();
        }
    }
}
```

### size 计算流程

size 计算实际发生在 put，remove 改变集合元素的操作之中

- 没有竞争发生，向 baseCount 累加计数
- 有竞争发生，新建 counterCells，向其中的一个 cell 累加计数
  - counterCells 初始有两个 cell
  - 如果计数竞争比较激烈，会创建新的 cell 来累加计数

```java
public int size() {
    long n = sumCount();
    return ((n < 0L) ? 0 :
            (n > (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE :
            (int)n);
}
final long sumCount() {
    CounterCell[] as = counterCells; CounterCell a;
    // 将 baseCount 计数与所有 cell 计数累加
    long sum = baseCount;
    if (as != null) {
        for (int i = 0; i < as.length; ++i) {
            if ((a = as[i]) != null)
                sum += a.value;
        }
    }
    return sum;
}
```

### 总结

Java 8 数组（Node） +（ 链表 Node | 红黑树 TreeNode ） 以下数组简称（table），链表简称（bin）

- 初始化，使用 cas 来保证并发安全，懒惰初始化 table
- 树化，当 table.length < 64 时，先尝试扩容，超过 64 时，并且 bin.length > 8 时，会将链表树化，树化过程会用 synchronized 锁住链表头
- put，如果该 bin 尚未创建，只需要使用 cas 创建 bin；如果已经有了，锁住链表头进行后续 put 操作，元素添加至 bin 的尾部
- get，无锁操作仅需要保证可见性，扩容过程中 get 操作拿到的是 ForwardingNode 它会让 get 操作在新 table 进行搜索
- 扩容，扩容时以 bin 为单位进行，需要对 bin 进行 synchronized，但这时妙的是其它竞争线程也不是无事可做，它们会帮助把其它 bin 进行扩容，扩容时平均只有 1/6 的节点会把复制到新 table 中
- size，元素个数保存在 baseCount 中，并发时的个数变动保存在 CounterCell[] 当中。最后统计数量时累加即可

> HashMap 允许有空 key 空 value，也就是可以为 null；但是 ConcurrentHashMap 不允许，会抛异常。

[程序员小灰——什么是 ConcurrentHashMap （JDK7）](https://mp.weixin.qq.com/s/1yWSfdz0j-PprGkDgOomhQ)

## Hashtable vs ConcurrentHashMap

**Hashtable 对比 ConcurrentHashMap**

* Hashtable 与 ConcurrentHashMap 都是线程安全的 Map 集合，它们的键值都是不能为空的。
* Hashtable 并发度低，整个 Hashtable 对应一把锁，同一时刻，只能有一个线程操作它
* ConcurrentHashMap 并发度高，整个 ConcurrentHashMap 对应多把锁，只要线程访问的是不同锁，那么不会冲突

**ConcurrentHashMap 1.7**

* 数据结构：`Segment(大数组) + HashEntry(小数组) + 链表`，每个 Segment 对应一把锁，如果多个线程访问不同的 Segment，则不会冲突
* 并发度：Segment 数组大小即并发度，决定了同一时刻最多能有多少个线程并发访问。Segment 数组不能扩容，意味着并发度在 ConcurrentHashMap 创建时就固定了
* 索引计算
  * 假设大数组长度是 $2^m$，key 在大数组内的索引是 key 的二次 hash 值的高 m 位
  * 假设小数组长度是 $2^n$，key 在小数组内的索引是 key 的二次 hash 值的低 n 位
* 扩容：每个小数组的扩容相对独立，小数组在超过扩容因子时会触发扩容，每次扩容翻倍
* Segment[0] 原型：首次创建其它小数组时，会以此原型为依据，数组长度，扩容因子都会以原型为准

**ConcurrentHashMap 1.8**

* 数据结构：`Node 数组 + 链表或红黑树`，每个链表的头节点作为锁，如果多个线程访问的头节点不同，则不会冲突。首次生成头节点时如果发生竞争，利用 cas 而非 syncronized，进一步提升性能
* 并发度：Node 数组有多大，并发度就有多大，与 1.7 不同，Node 数组可以扩容，数组的容量决定了它的并发度。
* 扩容条件：Node 数组满 3/4 时就会扩容
* 扩容单位：以链表为单位从后向前迁移链表，迁移完成的将旧数组头节点替换为 ForwardingNode
* 扩容时并发 get
  * 根据是否为 ForwardingNode 来决定是在新数组查找还是在旧数组查找，不会阻塞
  * 如果链表长度超过 1，则需要对节点进行复制（创建新节点），怕的是节点迁移后 next 指针改变
  * 如果链表最后几个元素扩容后索引不变，则节点无需复制
* 扩容时并发 put
  * 如果 put 的线程与扩容线程操作的链表是同一个，put 线程会阻塞
  * 如果 put 的线程操作的链表还未迁移完成，即头节点不是 ForwardingNode，则可以并发执行
  * 如果 put 的线程操作的链表已经迁移完成，即头结点是 ForwardingNode，则可以协助扩容
* 与 1.7 相比是懒惰初始化
* capacity 代表预估的元素个数，capacity / factor 来计算出初始数组大小，需要贴近 $2^n$ 
* loadFactor 只在计算初始数组大小时被使用，之后扩容固定为 3/4
* 超过树化阈值时的扩容问题，如果容量已经是 64，直接树化，否则在原来容量基础上做 3 轮扩容

## LinkedBlockingQueue

### 基本的入队出队

```java
public class LinkedBlockingQueue<E> extends AbstractQueue<E>
    implements BlockingQueue<E>, java.io.Serializable {
    static class Node<E> {
        E item;
        /**
     * 下列三种情况之一
     * - 真正的后继节点
     * - 自己, 发生在出队时
     * - null, 表示是没有后继节点, 是最后了
     */
        Node<E> next;
        Node(E x) { item = x; }
    }
}
```

初始化链表 `last = head = new Node<E>(null);` Dummy 节点用来占位，item 为 null

![](https://blog-images-erdochan.oss-cn-beijing.aliyuncs.com/CDN2/concurrent/image/image_95.png)

当一个节点入队 `last = last.next = node;`

![](https://blog-images-erdochan.oss-cn-beijing.aliyuncs.com/CDN2/concurrent/image/image_96.png)

再来一个节点入队 `last = last.next = node;`

![](https://blog-images-erdochan.oss-cn-beijing.aliyuncs.com/CDN2/concurrent/image/image_97.png)

出队

```java
Node<E> h = head;
Node<E> first = h.next;
h.next = h; // help GC 自己指向自己，不在被引用，方便垃圾回收
head = first;
E x = first.item;
first.item = null;
return x;
```

`h = head`

![](https://blog-images-erdochan.oss-cn-beijing.aliyuncs.com/CDN2/concurrent/image/image_98.png)

`first = h.next`

![](https://blog-images-erdochan.oss-cn-beijing.aliyuncs.com/CDN2/concurrent/image/image_99.png)

`h.next = h`

![](https://blog-images-erdochan.oss-cn-beijing.aliyuncs.com/CDN2/concurrent/image/image_100.png)

`head = first`

![](https://blog-images-erdochan.oss-cn-beijing.aliyuncs.com/CDN2/concurrent/image/image_101.png)

```java
// Dummy节点就是占位节点，是空的没有数据的，真正的数据在第一个Node
E x = first.item;
first.item = null;
return x;
```

![](https://blog-images-erdochan.oss-cn-beijing.aliyuncs.com/CDN2/concurrent/image/image_102.png)

### 加锁分析

**==高明之处==**在于用了两把锁和 dummy 节点，占位的头节点，是空的，这个和研究 ReentrantLock 底层时阻塞队列的占位 Dummy 节点是一样的，都是空的占位节点，也叫哨兵节点。

- 用一把锁，同一时刻，最多只允许有一个线程（生产者或消费者，二选一）执行
- 用两把锁，同一时刻，可以允许两个线程同时（一个生产者与一个消费者）执行
  - 消费者与消费者线程仍然串行
  - 生产者与生产者线程仍然串行
  - 保证消费者和消费者间的互斥，生产者与生产者间的互斥就行

线程安全分析

- 当节点总数大于 2 时（包括 dummy 节点），putLock 保证的是 last 节点的线程安全，takeLock 保证的是 head 节点的线程安全。两把锁保证了入队和出队没有竞争
- 当节点总数等于 2 时（即一个 dummy 节点，一个正常节点）这时候，仍然是两把锁锁两个对象，不会竞争，由此可见，Dummy 占位节点就是为了有两把锁对象，因为假设没有它的话，剩一个正常节点，是不可以有两把锁的，也没办法提升效率。
- 当节点总数等于 1 时（就一个 dummy 节点）这时 take 线程会被 notEmpty 条件阻塞，有竞争，会阻塞

```java
// 用于 put(阻塞) offer(非阻塞)
private final ReentrantLock putLock = new ReentrantLock();
// 用户 take(阻塞) poll(非阻塞)
private final ReentrantLock takeLock = new ReentrantLock();
```

#### put 操作

```java
public void put(E e) throws InterruptedException {
    if (e == null) throw new NullPointerException();
    int c = -1;
    Node<E> node = new Node<E>(e);
    final ReentrantLock putLock = this.putLock;
    // count 用来维护元素计数
    final AtomicInteger count = this.count;
    putLock.lockInterruptibly();
    try {
        // 满了等待
        while (count.get() == capacity) {
            // 倒过来读就好: 等待 notFull
            notFull.await();
        }
        // 有空位, 入队且计数加一
        enqueue(node);
        c = count.getAndIncrement(); 
        // 除了自己 put 以外, 队列还有空位, 由自己叫醒其他 put 线程
        if (c + 1 < capacity)
            notFull.signal();
    } finally {
        putLock.unlock();
    }
    // 如果队列中有一个元素, 叫醒 take 线程
    if (c == 0)
        // 这里调用的是 notEmpty.signal() 而不是 notEmpty.signalAll() 是为了减少竞争
        signalNotEmpty();
}
```

#### take 操作

```java
public E take() throws InterruptedException {
    E x;
    int c = -1;
    final AtomicInteger count = this.count;
    final ReentrantLock takeLock = this.takeLock;
    takeLock.lockInterruptibly();
    try {
        while (count.get() == 0) {
            notEmpty.await();
        }
        x = dequeue();
        c = count.getAndDecrement();
        if (c > 1)
            notEmpty.signal();
    } finally {
        takeLock.unlock();
    }
    // 如果队列中只有一个空位时, 叫醒 put 线程
    // 如果有多个线程进行出队, 第一个线程满足 c == capacity, 但后续线程 c < capacity
    if (c == capacity)
        // 这里调用的是 notFull.signal() 而不是 notFull.signalAll() 是为了减少竞争
        signalNotFull()
        return x;
}
```

> 由 put 唤醒 put 是为了避免信号不足

### 性能比较

主要列举 LinkedBlockingQueue 与 ArrayBlockingQueue 的性能比较

- Linked 支持有界，Array 强制有界
- Linked 实现是链表，Array 实现是数组
- Linked 是懒惰的，而 Array 需要提前初始化 Node 数组
- Linked 每次入队会生成新 Node，而 Array 的 Node 是提前创建好的
- Linked 两把锁，Array 一把锁

## ConcurrentLinkedQueue

ConcurrentLinkedQueue 的设计与 LinkedBlockingQueue 非常像，也是

- 两把【锁】，同一时刻，可以允许两个线程同时（一个生产者与一个消费者）执行
- dummy 节点的引入让两把【锁】将来锁住的是不同对象，避免竞争
- 只是这【锁】使用了 cas 来实现，LinkedBlockingQueue 的【锁】用的是 ReentrantLock

事实上，ConcurrentLinkedQueue 应用还是非常广泛的

例如之前讲的 Tomcat 的 Connector 结构时，Acceptor 作为生产者向 Poller 消费者传递事件信息时，正是采用了 ConcurrentLinkedQueue 将 SocketChannel 给 Poller 使用。

## CopyOnWriteArrayList

CopyOnWriteArraySet 是它的马甲 底层实现采用了 写入时拷贝 的思想，增删改操作会将底层数组拷贝一份，更改操作在新数组上执行，这时不影响其它线程的并发读，读写分离。 以新增为例：

```java
public boolean add(E e) {
    synchronized (lock) {
        // 获取旧的数组
        Object[] es = getArray();
        int len = es.length;
        // 拷贝新的数组（这里是比较耗时的操作，但不影响其它读线程）
        es = Arrays.copyOf(es, len + 1);
        // 添加新元素
        es[len] = e;
        // 替换旧的数组
        setArray(es);
        return true;
    }
}
```

> 这里的源码版本是 Java 11，在 Java 1.8 中使用的是可重入锁而不是 synchronized

其它读操作并未加锁，例如：

```java
public void forEach(Consumer<? super E> action) {
    Objects.requireNonNull(action);
    for (Object x : getArray()) {
        @SuppressWarnings("unchecked") E e = (E) x;
        action.accept(e);
    }
}
```

适合『读多写少』的应用场景，

### get 弱一致性

![](https://blog-images-erdochan.oss-cn-beijing.aliyuncs.com/CDN2/concurrent/image/image_103.png)

| 时间点 | 操作                         |
| ------ | ---------------------------- |
| 1      | Thread-0 getArray()          |
| 2      | Thread-1 getArray()          |
| 3      | Thread-1 setArray(arrayCopy) |
| 4      | Thread-0 array[index]        |

1 是能得到的。。。

> 不容易测试，但问题确实存在

### 迭代器弱一致性

```java
CopyOnWriteArrayList<Integer> list = new CopyOnWriteArrayList<>();
list.add(1);
list.add(2);
list.add(3);
Iterator<Integer> iter = list.iterator();
new Thread(() -> {
    list.remove(0);
    System.out.println(list);
}).start();
sleep1s();
while (iter.hasNext()) {
    System.out.println(iter.next());
}
```

就是怎么说呢。。读的可能是旧的，读写并发可能得到的是旧数据，适合读多写少，因为读的效率高，写的效率底，因为每次写都要复制一个新的数组来操作，这样成本高，并且写写是互斥的，CopyOnWriteArrayList 相当于用空间换线程安全。

这弱一致性，将读写分离，不光做到了读读并发，而且还做到了读写并发，只有写写互斥，这比之前的读写锁的效率更高。

>  不要觉得弱一致性就不好
>
>  - 数据库的 MVCC 都是弱一致性的表现
>  - 并发高和一致性是矛盾的，需要权衡



## ThreadLocalMap

ThreadLocalMap 不涉及到线程安全问题，因为是每个线程私有的，不会出现多个线程并发操作共享变量的情况。与 ThreadLocal 结合使用。

**作用**：

* ThreadLocal 可以实现【资源对象】的线程隔离，让每个线程各用各的【资源对象】，避免争用引发的线程安全问题
* ThreadLocal 同时实现了`线程内的`资源共享
* 线程之间可以实现【资源对象】的线程隔离，又实现了`线程内的`资源共享。线程间资源隔离，线程内资源共享。

**原理**：

每个线程内有一个 ThreadLocalMap 类型的成员变量，用来存储资源对象

* 调用 set 方法，就是以 ThreadLocal 自己作为 key，资源对象作为 value，放入当前线程的 ThreadLocalMap 集合中
* 调用 get 方法，就是以 ThreadLocal 自己作为 key，到当前线程中的 ThreadLocalMap 中查找关联的资源值
* 调用 remove 方法，就是以 ThreadLocal 自己作为 key，移除当前线程关联的资源值

ThreadLocalMap 的一些特点

* key 的 hash 值统一分配
* 初始容量 16，扩容因子 2/3，扩容容量翻倍
* key 索引冲突后用开放寻址法解决冲突

**弱引用 key**

ThreadLocalMap 中的 key 被设计为弱引用，原因如下

* Thread 可能需要长时间运行（如线程池中的线程），如果 key 不再使用，需要在内存不足（GC）时释放其占用的内存

但是 GC 仅仅是让 key 的内存释放，后续还要根据 key 是否为 null 来进一步释放值的内存，释放时机如下：

**内存释放时机**

* 被动 GC 释放 key
  * 仅是让 key 的内存释放，关联 value 的内存并不会释放
* 懒惰被动释放 value
  * get key 时，发现是 null key，则释放其 value 内存
  * set key 时，会使用启发式扫描，清除临近的 null key 的 value 内存，启发次数（临近几个）与元素个数、是否发现 null key 有关
* 主动 remove 释放 key，value
  * 会同时释放 key，value 的内存，也会清除临近的 null key 的 value 内存
  * 推荐使用它，因为一般使用 ThreadLocal 时都把它作为静态变量（即强引用），因此无法被动依靠 GC 回收，因此前两种用不上。

